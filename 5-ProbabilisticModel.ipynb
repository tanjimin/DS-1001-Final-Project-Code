{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/leo/Desktop/DSGA_1001/Project/data/data_train.csv')\n",
    "\n",
    "getDay = lambda x: dt.datetime.strptime(x[:-5], '%Y-%m-%d %H:%M:%S').day\n",
    "getHour = lambda x: dt.datetime.strptime(x[:-5], '%Y-%m-%d %H:%M:%S').hour\n",
    "\n",
    "data['Start Day'] = list(map(getDay, data['starttime']))\n",
    "data['Start Hour'] = list(map(getHour, data['starttime']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = pd.read_csv('/Users/leo/Desktop/DSGA_1001/Project/data/data_val.csv')\n",
    "\n",
    "getDay = lambda x: dt.datetime.strptime(x[:-5], '%Y-%m-%d %H:%M:%S').day\n",
    "getHour = lambda x: dt.datetime.strptime(x[:-5], '%Y-%m-%d %H:%M:%S').hour\n",
    "\n",
    "data_val['Start Day'] = list(map(getDay, data_val['starttime']))\n",
    "data_val['Start Hour'] = list(map(getHour, data_val['starttime']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['start region', 'start station latitude',\n",
    "            'start station longitude', 'usertype', 'birth year',\n",
    "            'gender', 'Start Day', 'Start Hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data_val.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_user = preprocessing.LabelEncoder()\n",
    "le_user.fit(data['usertype'])\n",
    "data['usertype'] = le_user.transform(data['usertype'])\n",
    "\n",
    "le_gender = preprocessing.LabelEncoder()\n",
    "le_gender.fit(data['gender'])\n",
    "data['gender'] = le_gender.transform(data['gender'])\n",
    "\n",
    "# le_start_station = preprocessing.LabelEncoder()\n",
    "# le_start_station.fit(data['start region'])\n",
    "# data['start region'] = le_start_station.transform(data['start region'])\n",
    "\n",
    "# le_end_station = preprocessing.LabelEncoder()\n",
    "# le_end_station.fit(data['end region'])\n",
    "# data['end region'] = le_end_station.transform(data['end region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_user1 = preprocessing.LabelEncoder()\n",
    "le_user1.fit(data_val['usertype'])\n",
    "data_val['usertype'] = le_user1.transform(data_val['usertype'])\n",
    "\n",
    "le_gender1 = preprocessing.LabelEncoder()\n",
    "le_gender1.fit(data_val['gender'])\n",
    "data_val['gender'] = le_gender1.transform(data_val['gender'])\n",
    "\n",
    "# le_start_station1 = preprocessing.LabelEncoder()\n",
    "# le_start_station1.fit(data1['start region'])\n",
    "# data1['start region'] = le_start_station1.transform(data1['start region'])\n",
    "\n",
    "# le_end_station1 = preprocessing.LabelEncoder()\n",
    "# le_end_station1.fit(data1['end region'])\n",
    "# data1['end region'] = le_end_station1.transform(data1['end region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[features]\n",
    "X_val = data_val[features]\n",
    "y_train = data['start region']\n",
    "y_val = data_val['end region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_withtime = X_train.copy()\n",
    "X_train_withtime['starttime'] = data['starttime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_withtime = X_val.copy()\n",
    "X_val_withtime['starttime'] = data_val['starttime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict = {}\n",
    "for i in range(X_train.shape[0]):\n",
    "    region_dict[X_train.iloc[i, 0]] = [X_train.iloc[i, 1], X_train.iloc[i, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Probabilistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=['start region', 'usertype', 'gender', 'start region',\n",
    "                                                               'Start Hour'], free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train, free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    " 'task': 'train',\n",
    " 'objective':'multiclass',\n",
    " 'metric': 'multi_logloss',\n",
    " 'num_class': 32,\n",
    " 'verbose': 1,\n",
    " 'max_bin': 128,  # 大会有更准的效果,更慢的速度\n",
    " 'learning_rate': 0.01,  # 学习率\n",
    " 'num_leaves': 64,  # 大会更准,但可能过拟合,\n",
    " 'num_iterations': 1000,\n",
    " 'early_stopping_round': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gbm = lgb.train(params,\n",
    "                lgb_train, valid_sets = [lgb_eval], verbose_eval = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption functions\n",
    "\n",
    "def get_duration(station1, station2, region_dict):\n",
    "    lat = region_dict[station1][0] - region_dict[station2][0]\n",
    "    long = region_dict[station1][1] - region_dict[station2][1]\n",
    "    return np.sqrt(lat**2 + long**2) * 1000\n",
    "\n",
    "def user_end_distribution(model, data):\n",
    "    # Station id and outflow percentage\n",
    "    return model.predict(data)\n",
    "\n",
    "# get data from data frame\n",
    "# return a list of all data\n",
    "# start_time_min/max must be datetime objects\n",
    "def get_unlock_data(start_time_min, start_time_max, start_station, full_data):\n",
    "    if type(start_time_min) != dt.datetime or type(start_time_max) != dt.datetime:\n",
    "        raise TypeError('Not datetime object.')\n",
    "    data_for_station = full_data[full_data['start region'] == start_station]\n",
    "    data_for_station['starttime'] = [dt.datetime.strptime(date[:-5], '%Y-%m-%d %H:%M:%S') for date in data_for_station['starttime']]\n",
    "    data_list = data_for_station[data_for_station['starttime'] > start_time_min]\n",
    "    data_list = data_list[data_list['starttime'] < start_time_max]\n",
    "    data_list = data_list.iloc[:, :-1]\n",
    "    return data_list\n",
    "\n",
    "# subtract duration(minutes) from time correclty\n",
    "def subtract_time(time, input_minutes):\n",
    "    if type(time) != dt.datetime:\n",
    "        raise TypeError('Not datetime object.')\n",
    "    datetime_minutes = timedelta(minutes = input_minutes)\n",
    "    adjusted_minutes = time - datetime_minutes\n",
    "    return adjusted_minutes\n",
    "\n",
    "# add duration(minutes) to time correclty\n",
    "def add_time(time, input_minutes):\n",
    "    if type(time) != dt.datetime:\n",
    "        raise TypeError('Not datetime object.')\n",
    "    datetime_minutes = timedelta(minutes = input_minutes)\n",
    "    adjusted_minutes = time + datetime_minutes\n",
    "    return adjusted_minutes\n",
    "\n",
    "# Compare time\n",
    "def smaller_than_time(time1, time2):\n",
    "    if type(time1) != dt.datetime or type(time2) != dt.datetime:\n",
    "        raise TypeError('Not datetime object.')\n",
    "    return time1 < time2\n",
    "\n",
    "def get_historical_inflow(start_station, end_station, end_time):\n",
    "    adjust_term = 5\n",
    "    return adjust_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_stations: a list of all stations\n",
    "# interval: mins (+- 10 mins)\n",
    "def calculate_inflow(predTime, curTime, end_station, all_stations, interval):\n",
    "    ## look up how to calculate time\n",
    "    # Calculate distribution for all stations\n",
    "    inflow = 0\n",
    "    ratio = 1\n",
    "    for start_station in all_stations:\n",
    "        trip_duration = get_duration(start_station, end_station, region_dict)\n",
    "        start_time = subtract_time(predTime, trip_duration)\n",
    "        # use historical data if now is earlier than start time:\n",
    "        if smaller_than_time(predTime, start_time):\n",
    "            inflow += get_historical_inflow(start_station, end_station, predTime)\n",
    "        else:\n",
    "            ratio = 30 / interval\n",
    "            min_time = subtract_time(start_time, interval / 2)\n",
    "            #print(min_time)\n",
    "            max_time = add_time(start_time, interval / 2)\n",
    "            #print(max_time)\n",
    "            unlock_list = get_unlock_data(min_time, \n",
    "                                          max_time,\n",
    "                                          start_station,\n",
    "                                          X_val_withtime)\n",
    "            # add inflow for every user from the same start station\n",
    "            for i in range(unlock_list.shape[0]):\n",
    "                unlocked_user = unlock_list.iloc[i,:]\n",
    "                proba = list(user_end_distribution(gbm, unlocked_user))[0]\n",
    "                inflow += proba[end_station]\n",
    "    inflow = inflow * ratio\n",
    "    return inflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate_inflow will return the predicted inflow at \"predTime\" for station \"station_idx\", assuming the current time is \"curTime\". The time window for collecting real data is determined by \"window\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTime = '2018-07-1 20:30:00.0700'\n",
    "station_idx = 11\n",
    "window = 10\n",
    "predTime = dt.datetime.strptime(predTime[:-5], '%Y-%m-%d %H:%M:%S')\n",
    "curTime = predTime - timedelta(minutes = 60)\n",
    "all_stations = list(range(1,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predflowlst = []\n",
    "for i in range(0,24):\n",
    "    predTime = '2018-07-15 {}:00:00.0700'.format(i)\n",
    "    station_idx = 11\n",
    "    window = 10\n",
    "    predTime = dt.datetime.strptime(predTime[:-5], '%Y-%m-%d %H:%M:%S')\n",
    "    curTime = predTime - timedelta(minutes = 60)\n",
    "    all_stations = list(range(1,32))\n",
    "    predflow = calculate_inflow(predTime, curTime, station_idx, all_stations, window)\n",
    "    predflowlst.append(predflow)\n",
    "    print(predflowlst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predflowlst2 = []\n",
    "for i in range(0,24):\n",
    "    predTime = '2018-07-20 {}:00:00.0700'.format(i)\n",
    "    station_idx = 11\n",
    "    window = 10\n",
    "    predTime = dt.datetime.strptime(predTime[:-5], '%Y-%m-%d %H:%M:%S')\n",
    "    curTime = predTime - timedelta(minutes = 60)\n",
    "    all_stations = list(range(1,32))\n",
    "    predflow = calculate_inflow(predTime, curTime, station_idx, all_stations, window)\n",
    "    predflowlst2.append(predflow)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
